{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5O3xz7oPosg"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;font-family:BNazanin,\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر فاطمه لشکری\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز اول پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۸ آبان ۱۴۰۰\n",
    "<br>\n",
    "<font size=4.8>\n",
    "دستیاران آموزشی مربوط به این فاز: سینا کاظمی - پارسا اسکندر\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i47HeS9NRU7B"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>\n",
    "مقدمه\n",
    "</h2>\n",
    "<p>\n",
    "هدف فاز اول پروژه پیاده سازی یک سیستم بازیابی اطلاعات است.\n",
    "این فاز پروژه از ۴ مرحله تشکیل شده است.\n",
    "بخش اول به پیش پردازش متنی داده ها می پردازد که شامل نرمال سازی متن (Normalization) ، جداسازی لغات (Tokenization) ، حذف لغات پرتکرار و ... می باشد. بخش دوم مربوط به نمایه سازی (indexing) خواهد بود. در بخش سوم باید روی این نمایه فشرده سازی صورت بگیرد. در بخش چهارم باید پرسمان ورودی کاربر را تصحیح کنید.\n",
    "</p>\n",
    "<p>\n",
    "۲ سری مجموعه دادگان در اختیار شما قرار گرفته است که یکی مربوط به زبان فارسی ( persian_dataset.csv ) و دیگری مربوط به زبان انگلیسی ( english_dataset.csv ) می باشد.\n",
    "مجموعه فارسی بخشی از مستندات ویکی پدیای فارسی می باشد و مجموعه انگلیسی مربوط به اخبار است.\n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fi2P02O_RviE"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>\n",
    "بخش اول ( پیش پردازش اولیه متن )\n",
    "</h2>\n",
    "<p>\n",
    "در این بخش از پروژه باید ابتدا مجموعه فایل هایی که در اختیارتان قرار گرفته است را بخوانید، برای خواندن و کار کردن با فایل‌ها پیشنهاد می‌شود از کتابخانه‌ی \n",
    "<a href=\"https://pandas.pydata.org/\">\n",
    "pandas\n",
    "</a> \n",
    "استفاده کنید.\n",
    "این کتابخانه توابعی برای \n",
    "<a href=\"https://www.geeksforgeeks.org/python-read-csv-using-pandas-read_csv/\">\n",
    "خواندن\n",
    "</a> \n",
    "و \n",
    "<a href=\"https://towardsdatascience.com/pandas-dataframe-basics-3c16eb35c4f3\">\n",
    "کار کردن\n",
    "</a> \n",
    "راحت‌تر با دیتاها در اختیار شما قرار می‌دهد .\n",
    "\n",
    "\n",
    " سپس به ترتیب مراحل پیش پردازش متنی که در ادامه آمده است را روی آنها اعمال کنید. برای کار های پیش رو می توانید از کتابخانه های \n",
    "<a href=\"https://www.nltk.org/\">\n",
    "NLTK\n",
    "</a>\n",
    " برای زبان انگلیسی و هضم ( \n",
    "   <a href=\"http://www.sobhe.ir/hazm/\">\n",
    "hazm\n",
    "</a> \n",
    " ) برای زبان فارسی استفاده کنید\n",
    "</p>\n",
    "<ul>\n",
    "<li>\n",
    "نرمال سازی متنی (normalization): برای یکسان سازی متون می توانید از توابع معرفی شده استفاده کنید. اما در صورتی که میخواهید خودتان پیاده سازی کنید باید پیاده \n",
    "سازیتان شامل برگرداندن لغات به ریشه (stemming)\n",
    "، case folding ( برای یکسان سازی متون انگلیسی ) و بقیه موارد مطرح شده در درس باشد.\n",
    "</li>\n",
    "<li>\n",
    "جداسازی (tokenization): برای اینکار میتوانید از کتابخانه های معرفی شده در بالا استفاده نمایید\n",
    "</li>\n",
    "<li>\n",
    "حذف علائم نگارشی: هر متنی در هر زبانی یه سری علائم نگارشی دارد که می بایست حذف شوند مانند: ویرگول ، دونقطه و ...\n",
    "</li>\n",
    "<li>\n",
    "یافتن و حذف لغات پرتکرار (stopwords): در این بخش، حذف درصد معقولی از لغات پرتکرار مورد نظر است. برای این منظور لازم است تا همه متن را پردازش کنید و نسبت به حجم متن، کلماتی که پرتکرار هستند را نمایش دهید. این نسبت را طوری در نظر بگیرید که کلمات پرتکرار به دست آمده تا حد خوبی منطقی و کافی باشند.\n",
    "</li>\n",
    "<li>\n",
    "برگرداندن کلمات به ریشه (stemming, lemmatizing): در نهایت کلمات را به حالت ساده و پایه آنها تبدیل کنید.\n",
    "</li>\n",
    "</ul>\n",
    "<p>\n",
    "برای پیاده سازی این بخش تابع پیاده سازی prepare_text را پر کنید.\n",
    "برای نمایش لغات پرتکرار می توانید از هیستوگرام و یا لیست ساده ای از کلمات استفاده کنید.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alireza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alireza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "#import hazm\n",
    "\n",
    "persian_dataset = pd.read_csv('persian_dataset.csv')\n",
    "english_dataset = pd.read_csv('english_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_3gsB7gzYIlo"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    \"\"\"Preprocesses the text with tokenization, case folding, stemming and lemmatization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_text : str\n",
    "        The title or plot of a movie\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of tokens\n",
    "    \"\"\"\n",
    "    without_punc = raw_text.translate(str.maketrans(\n",
    "        dict.fromkeys(list(string.punctuation))))\n",
    "    case_folded_tokens = [token.lower()\n",
    "                          for token in word_tokenize(without_punc)]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(\n",
    "        token, pos='v') for token in case_folded_tokens]\n",
    "    return lemmatized_tokens\n",
    "    # TODoO: tokenize, case_folding, stem, lemmatize\n",
    "    return [\"edvard\", \"am\", \"a\", \"run\", \"he\", \"am\", \"always\", \"run\"]\n",
    "\n",
    "\n",
    "prepare_text(\"Edvard was a Runner. He was always running.\")\n",
    "prepared_en_df = english_dataset[['title', 'text']].applymap(prepare_text)\n",
    "copy_of_prepared_en_df = prepared_en_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "0joZDmDIYLh8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 292382, 'be': 161301, 'to': 140960, 'of': 130650, 'and': 119905, 'a': 108585, 'in': 99170, 'that': 71419, '’': 59841, 'have': 58404, 'for': 47078, 'on': 41352, 'it': 36911, 's': 36599, '“': 35243}\n"
     ]
    }
   ],
   "source": [
    "terms = {}\n",
    "\n",
    "\n",
    "def count(element, dic):\n",
    "    if element in dic:\n",
    "        dic[element] += 1\n",
    "        return\n",
    "    else:\n",
    "        dic.update({element: 1})\n",
    "\n",
    "\n",
    "def get_stop_words():\n",
    "    repeat_titleList = {}\n",
    "    repeat_new_list = {}\n",
    "    repeat_overall = {}\n",
    "\n",
    "    for title in prepared_en_df['title']:\n",
    "        for element in title:\n",
    "            count(element, repeat_titleList)\n",
    "            count(element, repeat_overall)\n",
    "    for news_text in prepared_en_df['text']:\n",
    "        for element in news_text:\n",
    "            count(element, repeat_new_list)\n",
    "            count(element, repeat_overall)\n",
    "\n",
    "    stopOverall = sorted(repeat_overall.items(),\n",
    "                         key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    stopPlot = sorted(repeat_new_list.items(),\n",
    "                      key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    stopTitle = sorted(repeat_titleList.items(),\n",
    "                       key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "    thresh1 = 10\n",
    "    thresh2 = 25\n",
    "    thresh3 = 15\n",
    "\n",
    "    dic1 = {}\n",
    "    dic2 = {}\n",
    "    dic3 = {}\n",
    "    for i in range(0, thresh1):\n",
    "        dic1.update({stopTitle[i][0]: stopTitle[i][1]})\n",
    "    for i in range(0, thresh2):\n",
    "        dic2.update({stopPlot[i][0]: stopPlot[i][1]})\n",
    "    for i in range(0, thresh3):\n",
    "        dic3.update({stopOverall[i][0]: stopOverall[i][1]})\n",
    "\n",
    "    return dic3, repeat_overall\n",
    "\n",
    "\n",
    "stopWordList, terms = get_stop_words()\n",
    "\n",
    "print(stopWordList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_word(cell):\n",
    "    return [x for x in cell if x not in stopWordList]\n",
    "    \n",
    "prepared_en_df = prepared_en_df.applymap(remove_stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzpC6pjdSQ6U"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>\n",
    "بخش دوم ( نمایه سازی )\n",
    "</h2>\n",
    "<p>\n",
    "در این بخش پیاده سازی نمایه جایگاهی (Positional) و نمایه Bigram مطلوب است. برای نمایه جایگاهی باید به ازای هر لغت، لیستی از اسناد شامل آن لغت و جایگاه (ها) هر لغت در آن سند داشته باشید و برای نمایه Bigram نیز ترکیب های دوحرفی تمامی کلمات موجود در لغت نامه که این ترکیب در آنها موجود است را ذخیره کنید. این نمایه برای اصلاح پرسمان مورد استفاده قرار خواهد گرفت. نمایه شما باید پویا باشد یعنی با حذف سند از نمایه نیز حذف شود و با اضافه کردن سند یا اسناد در طول اجرای برنامه به نمایه اضافه شود. همچنین بعد از نمایه سازی باید قادر باشید نمایه را در فایلی ذخیره کرده و از آن بخوانید. پویا بودن نمایه و ذخیره سازی آن را برای هردو نوع نمایه در نظر بگیرید.\n",
    "</p>\n",
    "<p>\n",
    "نکات پیاده سازی:\n",
    "</br>\n",
    "برای سادگی پیاده سازی برای هر کارکرد خواسته شده در توضیحات بالا یک تابع پیاده سازی کنید. برای مثال دوتابع برای حذف و اضافه سند به نمایه و توابعی برای ذخیره سازی و بارگزاری نمایه ها و ... در نظر بگیرید.\n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToPositional(postings, docId, position):\n",
    "    if not postings or not postings[-1][0] == docId:\n",
    "        postings.append([docId, []])\n",
    "    postings[-1][1].append(position)\n",
    "    return postings\n",
    "\n",
    "\n",
    "def create_posting_dic(df):\n",
    "    posting_dics = {}\n",
    "    for index, row in df.iterrows():\n",
    "        title = row['title']\n",
    "        text = row['text']\n",
    "        addToPosting(title, index, title_posting_dics)\n",
    "        addToPosting(text, index, text_posting_dics)\n",
    "\n",
    "\n",
    "def addToPosting(tokens, docID, posting_dic):\n",
    "    for position in range(len(tokens)):\n",
    "        if not(tokens[position] in stopWordList.keys()):\n",
    "            if not(tokens[position] in posting_dic.keys()):\n",
    "                posting_dic.update({tokens[position]: []})\n",
    "            addToPositional(posting_dic[tokens[position]], docID, position)\n",
    "\n",
    "\n",
    "def create_positional_index():\n",
    "    title_posting_dics = {}\n",
    "    text_posting_dics = {}\n",
    "    create_posting_dic(prepared_en_df)\n",
    "    termDic = {}\n",
    "    for term in terms:\n",
    "        termDic.update({term: []})\n",
    "        if term in title_posting_dics:\n",
    "            termDic[term].append(title_posting_dics[term])\n",
    "        else:\n",
    "            termDic[term].append([])\n",
    "        if term in text_posting_dics:\n",
    "            termDic[term].append(text_posting_dics[term])\n",
    "        else:\n",
    "            termDic[term].append([])\n",
    "    return termDic\n",
    "\n",
    "\n",
    "positional_index = create_positional_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6330</th>\n",
       "      <td>[state, department, say, cant, find, email, fr...</td>\n",
       "      <td>[state, department, tell, republican, national...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>[‘, p, pbs, should, stand, ‘, plutocratic, or,...</td>\n",
       "      <td>[‘, p, pbs, should, stand, ‘, plutocratic, or,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6332</th>\n",
       "      <td>[antitrump, protesters, tool, oligarchy, infor...</td>\n",
       "      <td>[antitrump, protesters, tool, oligarchy, refo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>[ethiopia, obama, seek, progress, peace, secur...</td>\n",
       "      <td>[addis, ababa, ethiopia, —president, obama, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>[jeb, bush, suddenly, attack, trump, heres, wh...</td>\n",
       "      <td>[jeb, bush, suddenly, attack, trump, heres, wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "6330  [state, department, say, cant, find, email, fr...   \n",
       "6331  [‘, p, pbs, should, stand, ‘, plutocratic, or,...   \n",
       "6332  [antitrump, protesters, tool, oligarchy, infor...   \n",
       "6333  [ethiopia, obama, seek, progress, peace, secur...   \n",
       "6334  [jeb, bush, suddenly, attack, trump, heres, wh...   \n",
       "\n",
       "                                                   text  \n",
       "6330  [state, department, tell, republican, national...  \n",
       "6331  [‘, p, pbs, should, stand, ‘, plutocratic, or,...  \n",
       "6332  [antitrump, protesters, tool, oligarchy, refo...  \n",
       "6333  [addis, ababa, ethiopia, —president, obama, co...  \n",
       "6334  [jeb, bush, suddenly, attack, trump, heres, wh...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_doc_id = len(prepared_en_df)\n",
    "print(\"test\")\n",
    "\n",
    "\n",
    "prepared_en_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_en_df = copy_of_prepared_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_new_doc_id():\n",
    "    global last_doc_id\n",
    "    last_doc_id += 1\n",
    "    return last_doc_id-1\n",
    "\n",
    "\n",
    "def binary_search(arr, n):\n",
    "    low = 0\n",
    "    high = len(arr) - 1\n",
    "    mid = 0\n",
    "    while low <= high:\n",
    "        mid = (high + low) // 2\n",
    "        if arr[mid][0] < n:\n",
    "            low = mid + 1\n",
    "        elif arr[mid] > n:\n",
    "            high = mid - 1\n",
    "        else:\n",
    "            return mid\n",
    "    return -1\n",
    "\n",
    "\n",
    "def find_index(term, doc_ID, title_or_text):\n",
    "    binary_search(positional_index[term][title_or_text], doc_ID)\n",
    "\n",
    "\n",
    "def remove_document(doc_ID):\n",
    "    row = prepared_en_df.loc([doc_ID])\n",
    "    prepared_en_df.drop(doc_ID)\n",
    "    for term in row['title']:\n",
    "        del positional_index[term][0][find_index(term, doc_ID, 0)]\n",
    "    for term in row['text']:\n",
    "        del positional_index[term][1][find_index(term, doc_ID, 1)]\n",
    "\n",
    "\n",
    "def add_document(title, news):\n",
    "    title = prepare_text(title)\n",
    "    news = prepare_text(news)\n",
    "    docID = get_new_doc_id()\n",
    "    print(docID)\n",
    "    prepared_en_df.append(pd.Series({'title':title, 'text':news}, name=docID))\n",
    "    for position in range(len(title)):\n",
    "        if not title[position] in positional_index:\n",
    "            positional_index.update({title[position]: [[],[]]})\n",
    "        addToPositional(positional_index[title[position]][0], docID, position)\n",
    "    for position in range(len(news)):\n",
    "        if not news[position] in positional_index:\n",
    "            positional_index.update({news[position]: [[],[]]})\n",
    "        addToPositional(positional_index[news[position]][1], docID, position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6338, [1]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add_document(\"hello\",\"hi how are you?\")\n",
    "positional_index['how'][1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJBDwr7ZSQw8"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>\n",
    "بخش سوم ( فشرده سازی نمایه ها)\n",
    "</h2>\n",
    "در این بخش هدف، فشرده‌سازی نمایه‌های ساخته‌شده به دو روش \n",
    "variable code \n",
    "و\n",
    "gamma code \n",
    "است.\n",
    "\n",
    "<h3>\n",
    "نکات پیاده‌سازی \n",
    "</h3>\n",
    " هر دو روش پیاده‌سازی شود ولی برای ذخیره‌سازی نمایه‌ها در فایل و استفاده‌ از آن در بخش‌های بعد یکی از دو روش به دلخواه استفاده شود.\n",
    " <br>\n",
    "برای ذخیره نمایه در فایل می‌توانید از JSON \n",
    "یا\n",
    "pickle \n",
    "استفاده کنید.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqOITrpcU8ef",
    "outputId": "4a11351f-d214-495e-eb3f-7dc56863cd58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def store_index(path, compression_type):\n",
    "    \"\"\"Stores the index in a file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to store the file\n",
    "\n",
    "    compression_type : str\n",
    "        Could be one of the followings:\n",
    "        - no-compression\n",
    "        - gamma-code\n",
    "        - variable-byte\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The size of the stored file\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: compress and store positional_index\n",
    "    size_of_file = 1024\n",
    "    return size_of_file\n",
    "\n",
    "store_index(\"path/to/file.json\", \"no-compression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYN9I4_BD178",
    "outputId": "4a11351f-d214-495e-eb3f-7dc56863cd58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_index(path, compression_type):\n",
    "    \"\"\"Loads the index from a file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path of the file to load from\n",
    "\n",
    "    compression_type : str\n",
    "        Could be one of the followings:\n",
    "        - no-compression\n",
    "        - gamma-code\n",
    "        - variable-byte\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: load and decompress positional_index\n",
    "    return\n",
    "\n",
    "load_index(\"path/to/file.json\", \"no-compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mdCFLLySQab"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>\n",
    "بخش چهارم ( اصلاح پرسمان )\n",
    "</h2>\n",
    "در صورتی که پرسمان ورودی دارای غلط املایی باشد ( لغت‌ غلط لغتی است که در لغت‌نامه موجود نیست ) لازم است که با جست‌وجوی لغت‌های احتمالی و انتخاب بهترین لغت به ادامه‌ی جست‌وجو با پرسمان اصلاح‌شده پرداخته‌شود.\n",
    "برای این‌کار لازم است ابتدا به وسیله‌ی روش bigram \n",
    "و معیار jaccard \n",
    "نزدیک‌ترین لغات به لغت با غلط املایی را پیدا کنید و سپس بهترین لغت میان آن‌ها را با استفاده از معیار \n",
    "edit distance \n",
    "بیابید.\n",
    "<h3>\n",
    "نکات پیاده‌سازی:\n",
    "</h3>\n",
    "یک تابع پیاده‌سازی شود که ورودی خام را گرفته و متن تصحیح شده‌ی آن‌را نمایش دهد. دقت کنید ورودی و خروجی هر دو رشته‌ی متنی هستند.\n",
    "</br>\n",
    "نیازی به ذخیره‌سازی و فشرده‌سازی نمایه بایگرم نیست. همچنین می‌توانید از کد آماده برای محاسبه edit distance استفاده کنید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMZTstsbL1G3"
   },
   "outputs": [],
   "source": [
    "def create_bigram_index():\n",
    "    \"\"\"Creates the bigram index for spell correction\"\"\"\n",
    "\n",
    "    # TODO: create bigram index\n",
    "    bigram = {\"he\": [\"hello\", \"hell\", \"he\", \"her\"], \"el\": [\"telephone\", \"else\"]}\n",
    "    return bigram\n",
    "\n",
    "create_bigram_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sL3E8C-nL1yG",
    "outputId": "d4da236e-728c-4419-e80e-e8a36bd4e85d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'the adventure of sherlock holmes'"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_corrected_text(raw_text):\n",
    "    \"\"\"Corrects the query\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_text : strf\n",
    "        Input text that could be a title or a plot\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The corrected text\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: correct the query using bigram, jaccard, and edit-distance\n",
    "    corrected_text = \"the adventure of sherlock holmes\"\n",
    "    return corrected_text\n",
    "\n",
    "get_corrected_text(\"the adevntures of herlock holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epLKVSVaSPxN"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<h2>\n",
    "بخش پنجم ( نکات و جمع بندی پایانی )\n",
    "</h2>\n",
    "<ol>\n",
    "<li>\n",
    "گزارشی از پیاده سازی هر بخش تهیه در پایان هر بخش در همین فایل ژوپیتر قرار دهید.\n",
    "</li>\n",
    "<li>\n",
    "تنها زبان برنامه نویسی مجاز پایتون می باشد\n",
    "</li>\n",
    "<li>\n",
    "کد ها و توابع از نظر شباهت بررسی خواهد شد و با موارد مشابه و تقلب طبق آیین نامه تمارین درسی برخورد خواهد شد.\n",
    "</li>\n",
    "<li>\n",
    "فایل نوت بوک را زیپ کنید و با فرمت StudentNumber_phase1 ارسال نمایید.\n",
    "</li>\n",
    "<li>\n",
    "سیستم را بهینه پیاده سازی کنید تا در زمان کمتری بارگزاری و نمایه سازی شود.\n",
    "</li>\n",
    "<li>\n",
    "به ۵ پیاده سازی برتر نمره امتیازی تعلق میگیرد.\n",
    "</li>\n",
    "</ol>\n",
    "<p>\n",
    "\n",
    "در انجام پروژه اگر سوالی داشتید می توانید با ایمیل\n",
    " sinakazemi1998@gmail.com \n",
    " و\n",
    " parsa.eskandar@gmail.com\n",
    " در ارتباط باشید.\n",
    "</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MIR_phase1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
